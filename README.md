<h1 align="center" style="color:brown;">
  üêîü•öRobot Av√≠cola V2.0
</h1>

Proyecto de la PPS en Ingenier√≠a Mecatr√≥nica realizado en la FI-UNLZ.

Sistema rob√≥tico de 3 GDL para realizar la recolecci√≥n de huevos en nidos horizontales, tiene implementado un software con procesamiento de im√°genes para la detecci√≥n y localizaci√≥n en python utilizando la librer√≠a cv2 y un modelo pre-entrenado de roboflow. Se emplea IoT para el accionamiento de control del sistema, apliaci√≥n para el env√≠o de mensajes y recepci√≥n a trav√©s de "MQTT" adem√°s del monitoreo de variables ambientales c√≥mo la temperatura, gas CO2, h√∫medad.

## üü†Tecnolog√≠as Aplicadas
‚Ä¢ ESP32 - Shield cnc 3 ejes, para el control de los motores nema 17, motores el√©ctricos de cc y sensores.
‚Ä¢ Python - OpenCV, para el dise√±o de software de detecci√≥n y calculo de la cinematica inversa
‚Ä¢ Node-RED - MQTT, para el env√≠o y recepci√≥n de mensajes
‚Ä¢ Sensores de CO‚ÇÇ MQ135, temperatura y humedad DHT11

## üü†Prototipo REAL

<p align="center">
  <img src="20250718_190812.jpg" alt="Vista del robot" width="400"/>
</p>
<p align="center">
  <img src="gif.gif" alt="Vista del robot" width="400"/>
</p>

## üü†Modelo matem√°tico (Cinem√°tica inversa) Robot

En los siguientes √≠tems se describe el modelo cinem√°tico inverso aplicado al robot de 3GDL por m√©todo geom√©trico.

 üîπ Articulaci√≥n de la base $$\(\theta_1)$$

Este √°ngulo se obtiene a partir de la proyecci√≥n del punto de acci√≥n sobre el plano XY.  
Las rotaciones se consideran respecto al eje Z.

$Coordenadas (px, py)$


$$
\theta_1 = 2\arctan\left(\frac{py}{px}\right)
$$

üîπ Articulaci√≥n del brazo $$\(\theta_2)$$

$$
A_1 = l_1 + l_2 \cos(\theta_3), \quad A_2 = l_2 \sin(\theta_3)
$$

La distancia proyectada sobre el plano XY es:

$$
d_p = p_x \cos(\theta_1) + p_y \sin(\theta_1)
$$

Relaciones trigonom√©tricas:

$$
\sin(\theta_2) = \frac{p'_z A_1 - d_p A_2}{A_1^2 + A_2^2}
$$

$$
\cos(\theta_2) = \frac{d_p A_1 - p'_z A_2}{A_1^2 + A_2^2}
$$

Finalmente:

$$
\theta_2 = 2\arctan\left(\frac{\sin(\theta_2)}{\cos(\theta_2)}\right)
$$

üîπ  Articulaci√≥n del codo $$\(\theta_3)$$

Se ajusta en el plano vertical de la base, considerando la distancia entre el gripper y el extremo del codo.  

Paso1. Se calcula la coordenada corregida en Z:

$$\ p'_z = p_z - b \$$



Paso2. Distancia proyectada:

$$\ D^2 = p_x^2 + p_y^2 + (p'_z)^2 \$$


Paso3. Aplicando la ley de cosenos:

$$\cos(\theta_3) = \frac{D^2 - l_1^2 - l_2^2}{2 l_1 l_2}\$$


Paso4. √Ångulo del codo:


$$\theta_3 = 2\arctan\left(\frac{\sqrt{1-\cos^2(\theta_3)}}{\cos(\theta_3)}\right)\$$


Configuraci√≥n **codo arriba**


‚Ä¢ Con estos tres coordenadas articulares $$\(\theta_1, \theta_2, \theta_3)\$$ se obtiene la **configuraci√≥n del robot** para alcanzar cualquier punto dentro de su espacio de trabajo 17x13cm.


\section{Procesamiento de Im√°genes}

Para la detecci√≥n autom√°tica de huevos se utiliza un \textbf{modelo de visi√≥n por computadora} entrenado en la plataforma \textit{Roboflow}. El flujo de trabajo se resume en las siguientes etapas:

\begin{enumerate}
    \item \textbf{Captura de imagen} \\
    La c√°mara \textbf{ESP32-CAM} obtiene im√°genes en tiempo real desde la parte superior del nido, cubriendo toda la cuadr√≠cula donde se ubican los huevos.

    \item \textbf{Inferencia con el modelo} \\
    Las im√°genes capturadas son enviadas al modelo de Roboflow, el cual aplica un algoritmo de \textit{detecci√≥n de objetos} (YOLOv8). Como resultado, se obtiene un conjunto de predicciones que incluyen:
    \begin{itemize}
        \item Clases detectadas (ejemplo: ``huevo'').
        \item Confianza de detecci√≥n (probabilidad asociada a la predicci√≥n).
        \item Coordenadas del recuadro delimitador (\textit{bounding box}):
        \[
        (x, y, w, h)
        \]
        donde $x$ e $y$ representan la posici√≥n central del objeto, mientras que $w$ y $h$ corresponden al ancho y alto del recuadro.
    \end{itemize}

    \item \textbf{Conversi√≥n a coordenadas del robot} \\
    A partir de las coordenadas $(x, y)$ obtenidas en la cuadr√≠cula de la imagen (resoluci√≥n de $640 \times 480$ p√≠xeles), se realiza una transformaci√≥n a coordenadas f√≠sicas reales del robot:
    \[
    (X_r, Y_r) = f(x, y)
    \]
    donde la funci√≥n $f$ corresponde a la \textit{calibraci√≥n} que traduce los p√≠xeles en grados de los servomotores, garantizando que el brazo rob√≥tico pueda posicionarse correctamente sobre el huevo.

    \item \textbf{Selecci√≥n y validaci√≥n} \\
    Se consideran v√°lidas √∫nicamente las detecciones cuyo nivel de confianza sea mayor a un umbral predefinido (ejemplo: $0.75$). En caso de que un huevo ocupe m√°s de una celda de la cuadr√≠cula, se selecciona la celda con mayor proporci√≥n cubierta para evitar ambig√ºedad en la recolecci√≥n.

    \item \textbf{Env√≠o al sistema de control} \\
    Finalmente, las coordenadas corregidas $(X_r, Y_r)$ son transmitidas mediante el protocolo \textbf{MQTT} al controlador del robot, que ejecuta la secuencia de agarre y recolecci√≥n.
\end{enumerate}

\subsection*{Ejemplo de salida del modelo Roboflow}
Un ejemplo de predicci√≥n para un huevo detectado es el siguiente:
\begin{verbatim}
{
  "predictions": [
    {
      "class": "egg",
      "confidence": 0.89,
      "x": 320,
      "y": 240,
      "width": 50,
      "height": 60
    }
  ]
}
\end{verbatim}

Este resultado indica que se detect√≥ un \textbf{huevo} con una confianza del $89\%$ en la posici√≥n central $(320, 240)$ de la imagen. Posteriormente, este punto se transforma en coordenadas f√≠sicas para accionar el brazo rob√≥tico.



## üü†Video del Prototipo en funcionamiento

[Link de Google Drive](https://drive.google.com/file/d/1ZrKL6yfj1HvtoRiwAzGiAirnJcBjSen1/view?usp=drive_link)


<p align="center">
  <img src="Logo FIUNLZ.png" alt="Vista del robot" width="400"/>
</p>
