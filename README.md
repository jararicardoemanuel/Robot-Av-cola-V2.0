<h1 align="center" style="color:brown;">
  üêîü•öRobot Av√≠cola V2.0
</h1>

Proyecto de la PPS en Ingenier√≠a Mecatr√≥nica realizado en la FI-UNLZ.

Sistema rob√≥tico de 3 GDL para realizar la recolecci√≥n de huevos en nidos horizontales, tiene implementado un software con procesamiento de im√°genes para la detecci√≥n y localizaci√≥n en python utilizando la librer√≠a cv2 y un modelo pre-entrenado de roboflow. Se emplea IoT para el accionamiento de control del sistema, apliaci√≥n para el env√≠o de mensajes y recepci√≥n a trav√©s de "MQTT" adem√°s del monitoreo de variables ambientales c√≥mo la temperatura, gas CO2, h√∫medad.

## üü†Tecnolog√≠as Aplicadas
‚Ä¢ ESP32 - Shield cnc 3 ejes, para el control de los motores nema 17, motores el√©ctricos de cc y sensores.
‚Ä¢ Python - OpenCV, para el dise√±o de software de detecci√≥n y calculo de la cinematica inversa
‚Ä¢ Node-RED - MQTT, para el env√≠o y recepci√≥n de mensajes
‚Ä¢ Sensores de CO‚ÇÇ MQ135, temperatura y humedad DHT11

## üü†Prototipo REAL

<p align="center">
  <img src="20250718_190812.jpg" alt="Vista del robot" width="400"/>
</p>
<p align="center">
  <img src="gif.gif" alt="Vista del robot" width="400"/>
</p>


## üü†Procesamiento de Im√°genes

Para la detecci√≥n autom√°tica de huevos se utiliza un modelo de visi√≥n por computadora entrenado en la plataforma Roboflow. El flujo de trabajo se resume en las siguientes etapas:

‚Ä¢ Captura de imagen
  La c√°mara ESP32-CAM obtiene im√°genes en tiempo real desde la parte superior del nido, cubriendo toda la parte donde se ubican los huevos.
  
‚Ä¢ Inferencia con el modelo
  Las im√°genes capturadas son enviadas al modelo de Roboflow, el cual aplica un algoritmo de detecci√≥n de objetos (YOLOv9). Como resultado, se obtiene un conjunto de predicciones que      incluyen:
  Clases detectadas (``huevo'').
  Confianza de detecci√≥n (probabilidad asociada a la predicci√≥n).
  Coordenadas del recuadro delimitador:
  (x, y, w, h) donde $x$ e $y$ representan la posici√≥n central del objeto, mientras que $w$ y $h$ corresponden al ancho y alto del recuadro.

‚Ä¢ Conversi√≥n a coordenadas del robot
  A partir de las coordenadas $(x, y)$ obtenidas en la cuadr√≠cula de la imagen (resoluci√≥n de $640 \times 480$ p√≠xeles), se realiza una transformaci√≥n a coordenadas f√≠sicas reales del     robot:
    (X_r, Y_r) = f(x, y)
  donde la funci√≥n $f$ corresponde a la \textit{calibraci√≥n} que traduce los p√≠xeles en grados de los motores paso a paso, garantizando que el brazo rob√≥tico pueda posicionarse            correctamente sobre el huevo.

‚Ä¢ Selecci√≥n y validaci√≥n
  Se consideran v√°lidas √∫nicamente las detecciones cuyo nivel de confianza sea mayor a un umbral predefinido $0.75$. 
    
 ‚Ä¢ Env√≠o al sistema de control
  Finalmente, las coordenadas cartesianas corregidas $(X_r, Y_r)$ se calcula la cinem√°tica inversa del robot RRR para luego transmitir los angulos por el protocolo MQTT al controlador     del robot, que ejecuta la secuencia de agarre y recolecci√≥n.

  La salida del modelo Roboflow
  La predicci√≥n para un huevo detectado es el siguiente:
  "predictions": 
      "class": "egg",
      "confidence": 0.89,
      "x": 320,
      "y": 240,
      "width": 50,
      "height": 60
Este resultado indica que se detect√≥ un \textbf{huevo} con una confianza del $89%$ en la posici√≥n central $(320, 240)$. Posteriormente, este punto se transforma en coordenadas f√≠sicas para accionar el brazo rob√≥tico.

<p align="center">
  <img src="detecci√≥n.png" alt="Vista del robot" width="720"/>
</p>
<p align="center">
  <img src="coordenadas.png" alt="Vista del robot" width="720"/>
</p>
## üü†Modelo matem√°tico (Cinem√°tica inversa) Robot

En los siguientes √≠tems se describe el modelo cinem√°tico inverso aplicado al robot de 3GDL por m√©todo geom√©trico.

 üîπ Articulaci√≥n de la base $$\(\theta_1)$$

Este √°ngulo se obtiene a partir de la proyecci√≥n del punto de acci√≥n sobre el plano XY.  
Las rotaciones se consideran respecto al eje Z.

$Coordenadas (px, py)$


$$
\theta_1 = 2\arctan\left(\frac{py}{px}\right)
$$

üîπ Articulaci√≥n del brazo $$\(\theta_2)$$

$$
A_1 = l_1 + l_2 \cos(\theta_3), \quad A_2 = l_2 \sin(\theta_3)
$$

La distancia proyectada sobre el plano XY es:

$$
d_p = p_x \cos(\theta_1) + p_y \sin(\theta_1)
$$

Relaciones trigonom√©tricas:

$$
\sin(\theta_2) = \frac{p'_z A_1 - d_p A_2}{A_1^2 + A_2^2}
$$

$$
\cos(\theta_2) = \frac{d_p A_1 - p'_z A_2}{A_1^2 + A_2^2}
$$

Finalmente:

$$
\theta_2 = 2\arctan\left(\frac{\sin(\theta_2)}{\cos(\theta_2)}\right)
$$

üîπ  Articulaci√≥n del codo $$\(\theta_3)$$

Se ajusta en el plano vertical de la base, considerando la distancia entre el gripper y el extremo del codo.  

Paso1. Se calcula la coordenada corregida en Z:

$$\ p'_z = p_z - b \$$



Paso2. Distancia proyectada:

$$\ D^2 = p_x^2 + p_y^2 + (p'_z)^2 \$$


Paso3. Aplicando la ley de cosenos:

$$\cos(\theta_3) = \frac{D^2 - l_1^2 - l_2^2}{2 l_1 l_2}\$$


Paso4. √Ångulo del codo:


$$\theta_3 = 2\arctan\left(\frac{\sqrt{1-\cos^2(\theta_3)}}{\cos(\theta_3)}\right)\$$


Configuraci√≥n **codo arriba**


‚Ä¢ Con estos tres coordenadas articulares $$\(\theta_1, \theta_2, \theta_3)\$$ se obtiene la **configuraci√≥n del robot** para alcanzar cualquier punto dentro de su espacio de trabajo 17x13cm.



## üü†Video del Prototipo en funcionamiento

[Link de Google Drive](https://drive.google.com/file/d/1ZrKL6yfj1HvtoRiwAzGiAirnJcBjSen1/view?usp=drive_link)


<p align="center">
  <img src="Logo FIUNLZ.png" alt="Vista del robot" width="400"/>
</p>
